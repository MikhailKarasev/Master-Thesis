{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook implements the model given the already completed generator\n",
    "\n",
    "Author: Riaan Zoetmulder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from options import load_arguments\n",
    "from IO import create_embedding_layer, read_annotations, create_batches\n",
    "import tensorflow as tf\n",
    "from models import Generator\n",
    "from advanced_layers import RCNNCell\n",
    "import cPickle as pickle\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Recurrent Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtRCNNCell class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExtRCNNCell(RCNNCell):\n",
    "\n",
    "    def __call__(self, x, hc_tm1):\n",
    "        x_t, mask_t = x[0], x[1]\n",
    "        prevstate, hc_t  = super(ExtRCNNCell, self).__call__(x_t, hc_tm1)\n",
    "        #print 'shape of mask: ', mask_t.get_shape()\n",
    "        #print 'shape of hct: ', hc_t.get_shape()\n",
    "        #print 'shape of hctm1: ', hc_tm1.get_shape()\n",
    "        #print 'shape of prevstate: ', prevstate.get_shape()\n",
    "        a= mask_t * hc_t \n",
    "        b = (1-mask_t) * hc_tm1    \n",
    "        hc_t = a + b\n",
    "        return prevstate, hc_t\n",
    "\n",
    "\n",
    "    def copy_params(self, from_obj):\n",
    "        self.internal_layers = from_obj.internal_layers\n",
    "        self.bias = from_obj.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtLSTMCell class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Layer(x, n_classes,  hasbias = True, scope = None, act = tf.nn.sigmoid):\n",
    "    \n",
    "    with tf.variable_scope(scope or 'output_layer') as scope:\n",
    "        W = tf.get_variable('W_out', \n",
    "                            [x.get_shape()[1], n_classes],\n",
    "                            initializer = tf.contrib.layers.initializers.xavier_initializer())\n",
    "        \n",
    "\n",
    "        \n",
    "        temp = tf.matmul(x, W)\n",
    "        if hasbias:\n",
    "            \n",
    "            B = tf.get_variable('B_out', \n",
    "                                [1, n_classes],\n",
    "                                initializer = tf.constant_initializer(0.0))\n",
    "            \n",
    "            temp += B\n",
    "\n",
    "        logits = act(temp)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return logits\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(object):\n",
    "\n",
    "    def __init__(self, args, embedding_layer, nclasses, generator):\n",
    "        self.args = args\n",
    "        self.emb_layer = embedding_layer\n",
    "        self.nclasses = nclasses\n",
    "        self.gen = generator\n",
    "\n",
    "    def ready(self):\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            \n",
    "            gen = self.gen\n",
    "            emb_layer = self.emb_layer\n",
    "            args = self.args\n",
    "            padding_id = emb_layer.vocab_map[\"<padding>\"]\n",
    "            training = gen.training\n",
    "\n",
    "            # variables from the generator\n",
    "            dropout = gen.dropout\n",
    "            x = gen.x\n",
    "            z = gen.zpred\n",
    "            z = tf.expand_dims(z, 2)\n",
    "\n",
    "            # input placeholder\n",
    "            y = self.y = tf.placeholder(tf.float32, [None, self.nclasses], name= 'target_values')\n",
    "\n",
    "            n_d = args.hidden_dimension\n",
    "            n_e = emb_layer.n_d\n",
    "\n",
    "            layers = self.layers = [ ]\n",
    "            zero_states = self.zero_states = [ ]\n",
    "\n",
    "            depth = args.depth\n",
    "            use_all = args.use_all\n",
    "            layer_type = args.layer.lower()\n",
    "\n",
    "            # create layers\n",
    "            for i in xrange(depth):\n",
    "\n",
    "                # TODO: Include ExtLSTMCell here\n",
    "                layers.append(\n",
    "                                ExtRCNNCell(n_d,\n",
    "                                            idx = 'ExtRCNNCell_%i'%i)\n",
    "                             )\n",
    "                zero_states.append(\n",
    "                                    layers[i].zero_state(x.get_shape()[1])\n",
    "                                  )\n",
    "\n",
    "\n",
    "            # TODO: Some stuff missing here!\n",
    "\n",
    "            # create layers\n",
    "\n",
    "            h_prev = gen.rnn_inputs\n",
    "            lst_states = []\n",
    "            # print 'z outside shape: ', z.get_shape()\n",
    "            # print 'zero_state outside shape: ', zero_state.get_shape()\n",
    "            # print 'embs: ', embs.get_shape()\n",
    "\n",
    "            for idx, layer in enumerate(layers):\n",
    "\n",
    "                # a bug might occur here because you are using the same names for hnext t and t+1 \n",
    "                h_next, _=  tf.nn.dynamic_rnn(layer,\n",
    "                                          (h_prev, z),\n",
    "                                          initial_state= zero_states[idx], \n",
    "                                          time_major = True)\n",
    "                print 'layer ' + str(idx)+ ' ', h_next.get_shape()\n",
    "\n",
    "                ############################\n",
    "                # TODO: if pooling do stuff#\n",
    "                ############################\n",
    "                if args.pooling:\n",
    "                    # do something\n",
    "                    print 'implement the pooling'\n",
    "                    raise NotImplemented\n",
    "\n",
    "                else:\n",
    "                    lst_states.append(h_next[-1])\n",
    "\n",
    "                # update next state, apply dropout\n",
    "                h_prev = tf.cond(training,\n",
    "                             lambda: tf.nn.dropout(h_next, dropout), \n",
    "                             lambda: h_next, name='dropout_h_next')\n",
    "\n",
    "            # select whether to use all of them or not.\n",
    "            if args.use_all:\n",
    "                size = depth * n_d\n",
    "\n",
    "                # batch * size (i.e. n_d*depth)\n",
    "                h_final = tf.concat(1, lst_states)\n",
    "            else:\n",
    "                size = n_d\n",
    "                h_final = lst_states[-1]\n",
    "\n",
    "            # apply dropout to final state\n",
    "            h_final = tf.cond(training,\n",
    "                             lambda: tf.nn.dropout(h_final, dropout), \n",
    "                             lambda: h_final, name='dropout_h_next')\n",
    "\n",
    "\n",
    "            print h_final.get_shape()\n",
    "            # implement final layer\n",
    "            preds = self.preds = Layer(h_final, self.nclasses)\n",
    "\n",
    "            print 'preds: ', preds.get_shape()\n",
    "            loss_mat = self.loss_mat = (preds-y)**2 # batch\n",
    "\n",
    "            # difference in predicitons\n",
    "            pred_diff = self.pred_diff = tf.reduce_mean(tf.reduce_max(preds, 1) - tf.reduce_min(preds, 1))\n",
    "\n",
    "            # get the loss for each class\n",
    "            if args.aspect < 0:\n",
    "                loss_vec = tf.reduce_mean(loss_mat, 1)\n",
    "            else:\n",
    "                assert args.aspect < self.nclasses\n",
    "\n",
    "                loss_vec = loss_mat[:,args.aspect]\n",
    "\n",
    "            self.loss_vec = loss_vec\n",
    "\n",
    "            # get values from the generator\n",
    "            zsum = gen.zsum\n",
    "            zdiff = gen.zdiff\n",
    "            logpz = gen.logpz\n",
    "\n",
    "\n",
    "            coherent_factor = args.sparsity * args.coherent\n",
    "            # total loss\n",
    "            loss = self.loss = tf.reduce_mean(loss_vec)\n",
    "\n",
    "            # calculate the sparsity cost\n",
    "            sparsity_cost = self.sparsity_cost = tf.reduce_mean(zsum) * args.sparsity + \\\n",
    "                                                 tf.reduce_mean(zdiff) * coherent_factor\n",
    "\n",
    "            # loss function as mentioned in the paper\n",
    "            cost_vec = loss_vec + zsum * args.sparsity + zdiff * coherent_factor\n",
    "\n",
    "            cost_logpz = tf.reduce_mean(cost_vec * tf.reduce_sum(logpz, 0))\n",
    "            self.obj = tf.reduce_mean(cost_vec)\n",
    "\n",
    "        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Encoder')\n",
    "        \n",
    "        total_parameters = 0\n",
    "        for variable in variables:\n",
    "            sh = variable.get_shape()\n",
    "            variable_parametes = 1\n",
    "            for dim in sh:\n",
    "                variable_parametes *= dim.value\n",
    "            total_parameters += variable_parametes\n",
    "        print 'total # Encoder parameters:', total_parameters\n",
    "        \n",
    "        \n",
    "        # theano code\n",
    "        \n",
    "        lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in variables\n",
    "                    if 'bias' not in v.name ]) * self.args.l2_reg\n",
    "        \n",
    "        # generator and encoder loss\n",
    "        self.cost_g = cost_logpz * 10 + gen.L2_loss\n",
    "        self.cost_e = loss * 10 + lossL2\n",
    "        \n",
    "        print 'initialized!'\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class to be completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self, args, embedding_layer, nclasses):\n",
    "        self.args = args\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.nclasses = nclasses\n",
    "\n",
    "\n",
    "    def ready(self):\n",
    "        args, embedding_layer, nclasses = self.args, self.embedding_layer, self.nclasses\n",
    "        self.generator = Generator(args, nclasses, embedding_layer)\n",
    "        self.encoder = Encoder(args, embedding_layer, nclasses, self.generator)\n",
    "        \n",
    "        self.generator.ready()\n",
    "        self.encoder.ready()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147759 pre-trained embeddings loaded.\n"
     ]
    }
   ],
   "source": [
    "# load word embeddings once:\n",
    "\n",
    "embed_layer = create_embedding_layer(\n",
    "                                    'data/review+wiki.filtered.200.txt.gz'\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "\n",
    "def main(args, embed_layer):\n",
    "    print 'Parser Arguments' \n",
    "    \n",
    "    for key,value in args.__dict__.iteritems():\n",
    "        print u'{0}: {1}'.format(key, value)\n",
    "        \n",
    "        \n",
    "    # ensure embeddings exist\n",
    "    assert args.embedding, \"Pre-trained word embeddings required.\"\n",
    "    \n",
    "    max_len = args.max_len\n",
    "    \n",
    "    if args.train:\n",
    "        train_x, train_y = read_annotations(args.train)\n",
    "        train_x = [ embed_layer.map_to_ids(x)[:max_len] for x in train_x ]\n",
    "                   \n",
    "    # TODO: create development and test sets and rationale stuff\n",
    "            \n",
    "\n",
    "    if args.train:\n",
    "        with tf.Session() as sess:\n",
    "            # initialize Model\n",
    "            #TODO: create encoder class in model\n",
    "            \n",
    "            model = Model(\n",
    "                        args = args,\n",
    "                        embedding_layer = embed_layer,\n",
    "                        nclasses = len(train_y[0])\n",
    "                    )\n",
    "            model.ready()\n",
    "            \n",
    "            # added this for testing\n",
    "            # TODO: Remove later\n",
    "            train_batches_x, train_batches_y = create_batches(\n",
    "                                train_x, train_y, args.batch, model.generator.padding_id\n",
    "                            )\n",
    "            \n",
    "            feed_dict={model.generator.x: train_batches_x[0],model.encoder.y : train_batches_y[0], model.generator.embedding_placeholder: embed_layer.params[0], \n",
    "                      model.generator.dropout: 0.5, model.generator.training: True}\n",
    "                      \n",
    "            \n",
    "            init = tf.initialize_all_variables()\n",
    "            \n",
    "            \n",
    "            sess.run(init)\n",
    "            print 'past graph initialization'\n",
    "            \n",
    "            \n",
    "            cost_g = sess.run(model.encoder.cost_g , feed_dict)\n",
    "            \n",
    "            print cost_g\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser Arguments\n",
      "layer: rcnn\n",
      "aspect: -1\n",
      "dump: \n",
      "fix_emb: 1\n",
      "save_model: \n",
      "batch: 256\n",
      "max_len: 256\n",
      "use_all: 1\n",
      "learning: adam\n",
      "max_epochs: 100\n",
      "load_rationale: \n",
      "l2_reg: 1e-06\n",
      "sparsity: 0.0003\n",
      "decay_lr: 1\n",
      "load_model: \n",
      "beta2: 0.999\n",
      "beta1: 0.9\n",
      "test: \n",
      "activation: tanh\n",
      "learning_rate: 0.0005\n",
      "hidden_dimension: 200\n",
      "coherent: 2.0\n",
      "train: data/reviews.aspect1.train.txt.gz\n",
      "dropout: 0.1\n",
      "eval_period: -1\n",
      "dev: \n",
      "pooling: 0\n",
      "depth: 2\n",
      "hidden_dimension2: 30\n",
      "embedding: data/review+wiki.filtered.200.txt.gz\n",
      "order: 2\n",
      "70000 examples loaded from data/reviews.aspect1.train.txt.gz\n",
      "max text length: 1145\n",
      "Received dictionary of vocab size 147761 and embedding dim 200.\n",
      "shape of z: (?, 256, 1)\n",
      "total #  Generator parameters: 358281\n",
      "layer 0  (?, 256, 200)\n",
      "layer 1  (?, 256, 200)\n",
      "(256, 400)\n",
      "preds:  (256, 5)\n",
      "total # Encoder parameters: 322805\n",
      "initialized!\n",
      "past graph initialization\n",
      "-48.0557\n"
     ]
    }
   ],
   "source": [
    "args = pickle.load( open( \"data/args.p\", \"rb\" ) )\n",
    "reset_graph()\n",
    "main(args, embed_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
